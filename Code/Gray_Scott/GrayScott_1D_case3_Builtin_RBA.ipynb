{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINN Solution of the Gray Scott PDEs\n",
    "\n",
    "This PyTorch code demonstrates the application of physically-informed neural networks (PINN) in the solution of a well-known Gray Scott PDEs with periodic boundary condition\n",
    "\\begin{aligned}\n",
    "  &u_t = \\epsilon_1\\Delta u + b(1 - u) - uv^2, \\quad (t, x) \\in [0, T]\\times[-L, L]\\\\\n",
    "  &v_t = \\epsilon_2\\Delta v -dv + uv^2, \\\\\n",
    "  &u(0, x) = u_0(x), \\quad v(0, x) = v_0(x), \\quad \\forall x \\in [-L, L] \\\\\n",
    "  &u(t, -L) = u(t, L), \\quad v(t, -L) = v(t, L), \\quad \\forall t \\in [0, T]\n",
    "\\end{aligned}\n",
    "where $\\epsilon_1, \\epsilon_2, b, d > 0$ are some parameters, and $[x_{\\min}, x_{\\max}]$ covers one full period.\n",
    "\n",
    "See [this link](https://www.chebfun.org/examples/pde/GrayScott.html) for a description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from itertools import chain\n",
    "from collections import OrderedDict\n",
    "from pyDOE import lhs\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.gridspec as gridspec\n",
    "import time\n",
    "# set the random seed\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figsize(scale, nplots = 1):\n",
    "    fig_width_pt = 390.0                          # Get this from LaTeX using \\the\\textwidth\n",
    "    inches_per_pt = 1.0/72.27                       # Convert pt to inch\n",
    "    golden_mean = (np.sqrt(5.0)-1.0)/2.0            # Aesthetic ratio (you could change this)\n",
    "    fig_width = fig_width_pt*inches_per_pt*scale    # width in inches\n",
    "    fig_height = nplots*fig_width*golden_mean       # height in inches\n",
    "    fig_size = [fig_width,fig_height]\n",
    "    return fig_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgf_with_latex = {                      # setup matplotlib to use latex for output\n",
    "    \"pgf.texsystem\": \"pdflatex\",        # change this if using xetex or lautex\n",
    "    \"text.usetex\": True,                # use LaTeX to write all text\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [],                   # blank entries should cause plots to inherit fonts from the document\n",
    "    \"font.sans-serif\": [],\n",
    "    \"font.monospace\": [],\n",
    "    \"axes.labelsize\": 10,               # LaTeX default is 10pt font.\n",
    "    \"font.size\": 10,\n",
    "    \"legend.fontsize\": 8,               # Make the legend/label fonts a little smaller\n",
    "    \"xtick.labelsize\": 8,\n",
    "    \"ytick.labelsize\": 8,\n",
    "    \"figure.figsize\": figsize(1.0),     # default fig size of 0.9 textwidth\n",
    "    \"pgf.preamble\": r\"\\usepackage[utf8x]{inputenc} \\usepackage[T1]{fontenc}\"\n",
    "    # use utf8 fonts becasue your computer can handle it :)\n",
    "    # plots will be generated using this preamble\n",
    "    }\n",
    "mpl.rcParams.update(pgf_with_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPS or CUDA or CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "#\n",
    "print(f\"Working on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Problem Definition\n",
    "\"\"\"\n",
    "# define grid for quadrature solution\n",
    "# the following parameters and initial condition/PBC are taken from the exmaple\n",
    "# in spin('GS') from chebfun\n",
    "epsilon1 = 1\n",
    "epsilon2 = 1e-2\n",
    "b = 2e-2\n",
    "d = 8.26e-2\n",
    "L = 50.0\n",
    "xlo = -L\n",
    "xhi = L\n",
    "period = xhi - xlo\n",
    "tlo = 0.0\n",
    "thi = 25.0\n",
    "pi_ten = torch.tensor(np.pi).float().to(device)\n",
    "L_ten = torch.tensor(L).float().to(device)\n",
    "u0 = lambda x: 1.0 - 0.5 * np.power(np.sin(np.pi * (x - L)/(2.0 * L)), 4.0)\n",
    "u0_ten = lambda x: 1.0 - 0.5 * torch.pow(torch.sin(pi_ten * (x - L_ten)/(2.0 * L_ten)), 4.0)\n",
    "v0 = lambda x: 0.25 * np.power(np.sin(np.pi * (x - L)/(2.0 * L)), 4.0)\n",
    "v0_ten = lambda x: 0.25 * torch.pow(torch.sin(pi_ten * (x - L_ten)/(2.0 * L_ten)), 4.0)\n",
    "# we need to prepare the true solution from chebfun "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physics-informed Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the deep neural network\n",
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(DNN, self).__init__()\n",
    "        # parameters\n",
    "        self.depth = len(layers) - 1\n",
    "        # set up layer order dict\n",
    "        self.activation = torch.nn.Tanh\n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1): \n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
    "            )\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "            \n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        # deploy layers\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the physics-guided neural network\n",
    "# we will use built-in transformation to make it \n",
    "# periodic by default\n",
    "class PhysicsInformedNN():\n",
    "    def __init__(self, period, m, X_PDE, layers, gamma, eta, c, epsilon1, epsilon2, b, d):\n",
    "        # Prepare the periodic layer\n",
    "        m_vec = np.expand_dims(np.arange(1, m + 1), axis = 0)\n",
    "        self.ms = torch.tensor(2.0 * np.pi/period * m_vec).float().to(device)\n",
    "        # PDE data, gradients will be computed on these points so requires_grad = True\n",
    "        self.t_PDE = torch.tensor(X_PDE[:, 0:1], requires_grad=True).float().to(device)\n",
    "        self.x_PDE = torch.tensor(X_PDE[:, 1:2], requires_grad=True).float().to(device)\n",
    "        N_PDE = X_PDE.shape[0]\n",
    "        self.lambdas1 = torch.tensor(np.zeros((N_PDE, 1))).float().to(device)\n",
    "        self.lambdas2 = torch.tensor(np.zeros((N_PDE, 1))).float().to(device)\n",
    "        self.gamma = torch.tensor(gamma).float().to(device)\n",
    "        self.eta = torch.tensor(eta).float().to(device)\n",
    "        self.c = torch.tensor(c).float().to(device)\n",
    "        # layers to build Neural Net\n",
    "        # changing the input variables for periodic layer\n",
    "        # m cosines, m sines, and 1 costant, and 1 t\n",
    "        layers[0] = int(2 * m + 2)\n",
    "        self.layers = layers\n",
    "        # equation related parameters\n",
    "        self.epsilon1 = epsilon1\n",
    "        self.epsilon2 = epsilon2\n",
    "        self.b = b\n",
    "        self.d = d\n",
    "        # deep neural networks\n",
    "        self.dnn = DNN(layers).to(device)    \n",
    "        # prepare the optimizer\n",
    "        self.optimizer_Adam = torch.optim.Adam(self.dnn.parameters(), lr = 1e-3)\n",
    "        self.optimizer_LBFGS = torch.optim.LBFGS(\n",
    "            self.dnn.parameters(), \n",
    "            lr=1.0, \n",
    "            max_iter=10000, \n",
    "            max_eval=5000, \n",
    "            history_size=50,\n",
    "            tolerance_grad=1e-7, \n",
    "            tolerance_change=1.0 * np.finfo(float).eps,\n",
    "            line_search_fn=\"strong_wolfe\"       # can be \"strong_wolfe\"\n",
    "        )        \n",
    "        self.iter = 0\n",
    "    # evaluater neural network\n",
    "    def NN_eval(self, t, x):  \n",
    "        x_trans = torch.matmul(x, self.ms)\n",
    "        NN = self.dnn(torch.cat([t, torch.ones_like(x), torch.cos(x_trans), torch.sin(x_trans)], dim = 1))\n",
    "        uNN = NN[:, 0][:, None]\n",
    "        vNN = NN[:, 1][:, None]\n",
    "        u0_torch = u0_ten(x)\n",
    "        v0_torch = v0_ten(x)\n",
    "        u = u0_torch + t * uNN\n",
    "        v = v0_torch + t * vNN\n",
    "        return u, v\n",
    "    # compute the PDE\n",
    "    def pde_eval(self, t, x):\n",
    "        \"\"\" The pytorch autograd version of calculating residual \"\"\"\n",
    "        u, v = self.NN_eval(t, x)\n",
    "        # compute the derivatives for u\n",
    "        u_t  = torch.autograd.grad(u,   t, grad_outputs = torch.ones_like(u), retain_graph = True, create_graph=True)[0]\n",
    "        u_x  = torch.autograd.grad(u,   x, grad_outputs = torch.ones_like(u), retain_graph = True, create_graph=True)[0]\n",
    "        u_xx = torch.autograd.grad(u_x, x, grad_outputs = torch.ones_like(u), retain_graph = True, create_graph=True)[0]\n",
    "        v_t  = torch.autograd.grad(v,   t, grad_outputs = torch.ones_like(v), retain_graph = True, create_graph=True)[0]\n",
    "        v_x  = torch.autograd.grad(v,   x, grad_outputs = torch.ones_like(v), retain_graph = True, create_graph=True)[0]\n",
    "        v_xx = torch.autograd.grad(v_x, x, grad_outputs = torch.ones_like(v), retain_graph = True, create_graph=True)[0]\n",
    "        Eq1  = u_t - self.epsilon1 * u_xx - self.b * (1.0 - u) + u * torch.pow(v, 2.0)\n",
    "        Eq2  = v_t - self.epsilon2 * v_xx + self.d * v - u * torch.pow(v, 2.0)\n",
    "        return Eq1, Eq2\n",
    "    # # compute the total loss for the second-order optimizer\n",
    "    # def loss_func(self):\n",
    "    #     # reset the gradient\n",
    "    #     self.optimizer_LBFGS.zero_grad()\n",
    "    #     #\n",
    "    #     lambdas1_prev = self.lambdas1\n",
    "    #     lambdas2_prev = self.lambdas2        \n",
    "    #     # compute PDE loss\n",
    "    #     pde1_pred, pde2_pred = self.pde_eval(self.t_PDE, self.x_PDE)\n",
    "    #     residual1 = torch.abs(pde1_pred)\n",
    "    #     lambdas1 = self.gamma * lambdas1_prev + self.eta * residual1/torch.max(residual1) + self.c\n",
    "    #     loss_PDE1 = torch.mean(torch.square(lambdas1 * pde1_pred))\n",
    "    #     residual2 = torch.abs(pde2_pred)\n",
    "    #     lambdas2 = self.gamma * lambdas2_prev + self.eta * residual2/torch.max(residual2) + self.c            \n",
    "    #     loss_PDE2 = torch.mean(torch.square(lambdas2 * pde2_pred))\n",
    "    #     lambdas1_prev = lambdas1\n",
    "    #     lambdas2_prev = lambdas2\n",
    "    #     # compute the total loss, it can be weighted\n",
    "    #     loss = loss_PDE1 + loss_PDE2\n",
    "    #     # backward propagation\n",
    "    #     loss.backward()\n",
    "    #     # increase the iteration counter\n",
    "    #     self.iter += 1\n",
    "    #     # output\n",
    "    #     # output the progress\n",
    "    #     if self.iter % 1000 == 0:\n",
    "    #         print('Iter %5d, Total: %10.4e' % (self.iter, loss.item()))\n",
    "    #         print('PDE(u): %10.4e, PDE(v): %10.4e' % (loss_PDE1.item(), loss_PDE2.item()))         \n",
    "    #     return loss\n",
    "    #\n",
    "    def train(self, nIter):\n",
    "        # start the training with Adam first\n",
    "        self.dnn.train()\n",
    "        lambdas1_prev = self.lambdas1\n",
    "        lambdas2_prev = self.lambdas2\n",
    "        for epoch in range(nIter):\n",
    "            # compute PDE loss\n",
    "            pde1_pred, pde2_pred = self.pde_eval(self.t_PDE, self.x_PDE)\n",
    "            residual1 = torch.abs(pde1_pred)\n",
    "            lambdas1 = self.gamma * lambdas1_prev + self.eta * residual1/torch.amax(residual1) + self.c\n",
    "            loss_PDE1 = torch.mean(torch.square(lambdas1 * pde1_pred))\n",
    "            residual2 = torch.abs(pde2_pred)\n",
    "            lambdas2 = self.gamma * lambdas2_prev + self.eta * residual2/torch.amax(residual2) + self.c            \n",
    "            loss_PDE2 = torch.mean(torch.square(lambdas2 * pde2_pred))\n",
    "            # compute the total loss, it can be weighted\n",
    "            loss = loss_PDE1 + loss_PDE2\n",
    "            # loss_PDE1.backward(retain_graph = True)\n",
    "            # loss_PDE2.backward(retain_graph = True)            \n",
    "            # Backward and optimize\n",
    "            self.optimizer_Adam.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer_Adam.step() \n",
    "            # keep \n",
    "            lambdas1_prev = lambdas1.clone().detach()\n",
    "            lambdas2_prev = lambdas2.clone().detach()            \n",
    "            # output the progress\n",
    "            if (epoch + 1) % 1000 == 0:\n",
    "                print('Iter %5d, Total: %10.4e' % (epoch + 1, loss.item()))\n",
    "                print('PDE(u): %10.4e, PDE(v): %10.4e' % (loss_PDE1.item(), loss_PDE2.item()))\n",
    "        # # Backward and optimize\n",
    "        # self.lambdas1 = lambdas1\n",
    "        # self.lambdas2 = lambdas2\n",
    "        # self.optimizer_LBFGS.step(self.loss_func)                \n",
    "    #        \n",
    "    def predict(self, X):\n",
    "        t = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device)\n",
    "        x = torch.tensor(X[:, 1:2], requires_grad=True).float().to(device)\n",
    "        self.dnn.eval()\n",
    "        u, v = self.NN_eval(t, x)\n",
    "        u = u.detach().cpu().numpy()\n",
    "        v = v.detach().cpu().numpy()\n",
    "        return u, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the points\n",
    "def get_points_for_training(tlo, thi, xlo, xhi, N_PDE):\n",
    "    # for collocation pts, (t, x) \\in (0, T)x\\Omega\n",
    "    pts_rand = lhs(2, N_PDE)\n",
    "    t_PDE = tlo + (thi - tlo) * pts_rand[:, 0:1]\n",
    "    x_PDE = xlo + (xhi - xlo) * pts_rand[:, 1:2]\n",
    "    ptsPDE = np.hstack((t_PDE, x_PDE))\n",
    "    return ptsPDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [2, 32, 32, 32, 32, 32, 32, 32, 2]\n",
    "m = 10\n",
    "gamma = 0.9\n",
    "eta = 1.0\n",
    "c = 1.0\n",
    "N_IC = 64\n",
    "N_PDE = int(64 * 64)\n",
    "ptsPDE = get_points_for_training(tlo, thi, xlo, xhi, N_PDE)\n",
    "model = PhysicsInformedNN(period, m, ptsPDE, layers, gamma, eta, c, epsilon1, epsilon2, b, d)\n",
    "model.train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply PINN to the same grid as the quadrature solution for comparison\n",
    "t = np.linspace(tlo, thi, 101)\n",
    "x = np.linspace(xlo, xhi, 101)\n",
    "T, X = np.meshgrid(t, x)\n",
    "pts_flat = np.hstack((T.flatten()[:, None], X.flatten()[:, None]))\n",
    "u_pred, v_pred = model.predict(pts_flat)\n",
    "# #\n",
    "# Exact = u_quad.T\n",
    "# Exact_vec = Exact.flatten()[:, None]\n",
    "# error_u = np.linalg.norm(Exact_vec-u_pred,2)/np.linalg.norm(Exact_vec,2)\n",
    "# print('Error u: %e' % (error_u))                     \n",
    "u_pred = griddata(pts_flat, u_pred.flatten(), (T, X), method='cubic')\n",
    "v_pred = griddata(pts_flat, v_pred.flatten(), (T, X), method='cubic')\n",
    "# Error = np.abs(Exact - U_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The aesthetic setting has changed. \"\"\"\n",
    "\n",
    "####### Row 0: u(t,x) ##################    \n",
    "\n",
    "fig = plt.figure(figsize=(11, 5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "#\n",
    "ax.plot(ptsPDE[:, 0], ptsPDE[:, 1], \n",
    "    'rd', label = 'PDE Data (%d points)' % (ptsPDE.shape[0]), \n",
    "    markersize = 4,  # marker size doubled\n",
    "    clip_on = False,\n",
    "    alpha=1.0\n",
    ")\n",
    "#\n",
    "ax.set_xlabel('$t$', size=15)\n",
    "ax.set_ylabel('$x$', size=15)\n",
    "ax.legend(\n",
    "    loc='upper center', \n",
    "    bbox_to_anchor=(0.9, -0.05), \n",
    "    ncol=5, \n",
    "    frameon=False, \n",
    "    prop={'size': 15}\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_title('Points', fontsize = 15) # font size doubled\n",
    "ax.tick_params(labelsize=12)\n",
    "#\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Row 1: u(t,x) slices ################## \n",
    "\n",
    "\"\"\" The aesthetic setting has changed. \"\"\"\n",
    "\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "#\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "#ax.plot(x, Exact[:, 25], 'bo-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(x, u_pred[:, 0], 'rx--', linewidth = 2, label = 'u')\n",
    "ax.plot(x, v_pred[:, 0], 'bo--', linewidth = 2, label = 'v')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')    \n",
    "ax.set_title('$t = %.1f$' %(t[0]), fontsize = 15)\n",
    "#ax.axis('square')\n",
    "# ax.set_xlim([-0.1,1.1])\n",
    "# ax.set_ylim([-0.1,1.1]) \n",
    "plt.locator_params(axis = 'y', nbins = 5)\n",
    "plt.locator_params(axis = 'x', nbins = 5)\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(15)\n",
    "\n",
    "ax = plt.subplot(1, 3, 2)\n",
    "#ax.plot(x,Exact[:,50], 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(x, u_pred[:, 50], 'rx--', linewidth = 2, label = 'u')\n",
    "ax.plot(x, v_pred[:, 50], 'bo--', linewidth = 2, label = 'v')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')\n",
    "#ax.axis('square')\n",
    "# ax.set_xlim([-L,L])\n",
    "# ax.set_ylim([-0.1,1.1]) \n",
    "ax.set_title('$t = %.1f$' %(t[50]), fontsize = 15)\n",
    "ax.legend(\n",
    "    loc='upper center', \n",
    "    bbox_to_anchor=(0.5, -0.15), \n",
    "    ncol=5, \n",
    "    frameon=False, \n",
    "    prop={'size': 15}\n",
    ")\n",
    "plt.locator_params(axis = 'y', nbins = 5)\n",
    "plt.locator_params(axis = 'x', nbins = 5)\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(15)\n",
    "\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "#ax.plot(x,Exact[:,75], 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(x, u_pred[:, 100], 'rx--', linewidth = 2, label = 'u')\n",
    "ax.plot(x, v_pred[:, 100], 'bo--', linewidth = 2, label = 'v')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')\n",
    "#ax.axis('square')\n",
    "# ax.set_xlim([-0.1,1.1])\n",
    "# ax.set_ylim([-0.1,1.1])    \n",
    "ax.set_title('$t = %.1f$' %(t[100]), fontsize = 15)\n",
    "plt.locator_params(axis = 'y', nbins = 5)\n",
    "plt.locator_params(axis = 'x', nbins = 5)\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
