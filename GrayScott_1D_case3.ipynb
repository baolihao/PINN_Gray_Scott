{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINN Solution of the Gray Scott PDEs\n",
    "\n",
    "This PyTorch code demonstrates the application of physically-informed neural networks (PINN) in the solution of a well-known Gray Scott PDEs with periodic boundary condition\n",
    "\\begin{aligned}\n",
    "  &u_t = \\epsilon_1\\Delta u + b(1 - u) - uv^2, \\quad (t, x) \\in [0, T]\\times[-L, L]\\\\\n",
    "  &v_t = \\epsilon_2\\Delta v -dv + uv^2, \\\\\n",
    "  &u(0, x) = u_0(x), \\quad v(0, x) = v_0(x), \\quad \\forall x \\in [-L, L] \\\\\n",
    "  &u(t, -L) = u(t, L), \\quad v(t, -L) = v(t, L), \\quad \\forall t \\in [0, T]\n",
    "\\end{aligned}\n",
    "where $\\epsilon_1, \\epsilon_2, b, d > 0$ are some parameters, and $[x_{\\min}, x_{\\max}]$ covers one full period.\n",
    "\n",
    "See [this link](https://www.chebfun.org/examples/pde/GrayScott.html) for a description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from itertools import chain\n",
    "from collections import OrderedDict\n",
    "from pyDOE import lhs\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.gridspec as gridspec\n",
    "import time\n",
    "# set the random seed\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figsize(scale, nplots = 1):\n",
    "    fig_width_pt = 390.0                          # Get this from LaTeX using \\the\\textwidth\n",
    "    inches_per_pt = 1.0/72.27                       # Convert pt to inch\n",
    "    golden_mean = (np.sqrt(5.0)-1.0)/2.0            # Aesthetic ratio (you could change this)\n",
    "    fig_width = fig_width_pt*inches_per_pt*scale    # width in inches\n",
    "    fig_height = nplots*fig_width*golden_mean       # height in inches\n",
    "    fig_size = [fig_width,fig_height]\n",
    "    return fig_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgf_with_latex = {                      # setup matplotlib to use latex for output\n",
    "    \"pgf.texsystem\": \"pdflatex\",        # change this if using xetex or lautex\n",
    "    \"text.usetex\": True,                # use LaTeX to write all text\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [],                   # blank entries should cause plots to inherit fonts from the document\n",
    "    \"font.sans-serif\": [],\n",
    "    \"font.monospace\": [],\n",
    "    \"axes.labelsize\": 10,               # LaTeX default is 10pt font.\n",
    "    \"font.size\": 10,\n",
    "    \"legend.fontsize\": 8,               # Make the legend/label fonts a little smaller\n",
    "    \"xtick.labelsize\": 8,\n",
    "    \"ytick.labelsize\": 8,\n",
    "    \"figure.figsize\": figsize(1.0),     # default fig size of 0.9 textwidth\n",
    "    \"pgf.preamble\": r\"\\usepackage[utf8x]{inputenc} \\usepackage[T1]{fontenc}\"\n",
    "    # use utf8 fonts becasue your computer can handle it :)\n",
    "    # plots will be generated using this preamble\n",
    "    }\n",
    "mpl.rcParams.update(pgf_with_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on mps\n"
     ]
    }
   ],
   "source": [
    "# MPS or CUDA or CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "#\n",
    "print(f\"Working on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Problem Definition\n",
    "\"\"\"\n",
    "# define grid for quadrature solution\n",
    "epsilon1 = 1\n",
    "epsilon2 = 1e-2\n",
    "b = 2e-2\n",
    "d = 8.26e-2\n",
    "L = 50.0\n",
    "xlo = -L\n",
    "xhi = L\n",
    "period = xhi - xlo\n",
    "tlo = 0.0\n",
    "thi = 80.0\n",
    "u0 = lambda x: 1 - 0.5 * np.power(np.sin(np.pi * (x - L)/(2.0 * L)), 100.0)\n",
    "v0 = lambda x: 0.25 * np.power(np.sin(np.pi * (x - L)/(2.0 * L)), 100.0)\n",
    "# we need to prepare the true solution from chebfun "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physics-informed Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the deep neural network\n",
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(DNN, self).__init__()\n",
    "        # parameters\n",
    "        self.depth = len(layers) - 1\n",
    "        # set up layer order dict\n",
    "        self.activation = torch.nn.Tanh\n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1): \n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
    "            )\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "            \n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        # deploy layers\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the physics-guided neural network\n",
    "class PhysicsInformedNN():\n",
    "    def __init__(self, X_data, u_data, v_data, X_pbc, period, X_pde, layers, epsilon1, epsilon2, b, d):\n",
    "        # data (Dirichelet BC or Initial Condition or noise data)\n",
    "        self.N_data = X_data.shape[0]\n",
    "        self.t_data = torch.tensor(X_data[:, 0:1]).float().to(device)\n",
    "        self.x_data = torch.tensor(X_data[:, 1:2]).float().to(device)\n",
    "        self.u_data = torch.tensor(u_data).float().to(device)\n",
    "        self.v_data = torch.tensor(v_data).float().to(device)\n",
    "        # Periodic BC data\n",
    "        self.N_pbc = X_pbc.shape[0]\n",
    "        self.t_pbc = torch.tensor(X_pbc[:, 0:1]).float().to(device)\n",
    "        self.x_pbc = torch.tensor(X_pbc[:, 1:2]).float().to(device)\n",
    "        self.period = torch.tensor(period).float().to(device)\n",
    "        # PDE data, gradients will be computed on these points so requires_grad = True\n",
    "        self.N_pde = X_pde.shape[0]\n",
    "        self.t_pde = torch.tensor(X_pde[:, 0:1], requires_grad=True).float().to(device)\n",
    "        self.x_pde = torch.tensor(X_pde[:, 1:2], requires_grad=True).float().to(device)\n",
    "        # layers to build Neural Net\n",
    "        self.layers = layers\n",
    "        # equation related parameters\n",
    "        self.epsilon1 = epsilon1\n",
    "        self.epsilon2 = epsilon2\n",
    "        self.b = b\n",
    "        self.d = d\n",
    "        # construct the loss weights\n",
    "        self.lws = torch.tensor(np.ones((self.N_data + self.N_pbc + self.N_pde, 1), dtype = np.single), requires_grad=True).to(device)\n",
    "        self.lws = torch.nn.Parameter(self.lws)\n",
    "        # deep neural networks\n",
    "        self.dnn = DNN(layers).to(device)\n",
    "        self.optimizer_Adam = torch.optim.Adam(self.dnn.parameters(), lr = 1e-3)\n",
    "        self.optimizer_lws = torch.optim.Adam(chain((self.lws, )), lr = 1e-3)\n",
    "        self.iter = 0\n",
    "    # evaluater neural network\n",
    "    def NN_eval(self, t, x):  \n",
    "        NN = self.dnn(torch.cat([t, x], dim = 1))\n",
    "        u = NN[:, 0][:, None]\n",
    "        v = NN[:, 1][:, None]\n",
    "        return u, v\n",
    "    # compute the PDE\n",
    "    def pde_eval(self, t, x):\n",
    "        \"\"\" The pytorch autograd version of calculating residual \"\"\"\n",
    "        u, v = self.NN_eval(t, x)\n",
    "        # compute the derivatives for u\n",
    "        u_t  = torch.autograd.grad(u,   t, grad_outputs = torch.ones_like(u), retain_graph = True, create_graph=True)[0]\n",
    "        u_x  = torch.autograd.grad(u,   x, grad_outputs = torch.ones_like(u), retain_graph = True, create_graph=True)[0]\n",
    "        u_xx = torch.autograd.grad(u_x, x, grad_outputs = torch.ones_like(u), retain_graph = True, create_graph=True)[0]\n",
    "        v_t  = torch.autograd.grad(v,   t, grad_outputs = torch.ones_like(v), retain_graph = True, create_graph=True)[0]\n",
    "        v_x  = torch.autograd.grad(v,   x, grad_outputs = torch.ones_like(v), retain_graph = True, create_graph=True)[0]\n",
    "        v_xx = torch.autograd.grad(v_x, x, grad_outputs = torch.ones_like(v), retain_graph = True, create_graph=True)[0]\n",
    "        Eq1  = u_t - self.epsilon1 * u_xx - self.b * (1.0 - u) + u * torch.pow(v, 2.0)\n",
    "        Eq2  = v_t - self.epsilon2 * v_xx + self.d * v - u * torch.pow(v, 2.0)\n",
    "        return Eq1, Eq2\n",
    "    # # compute the total loss for the second-order optimizer\n",
    "    # def loss_func(self):\n",
    "    #     # reset the gradient\n",
    "    #     self.optimizer.zero_grad()\n",
    "    #     # compute the data loss\n",
    "    #     u_pred, v_pred = self.NN_eval(self.t_data, self.x_data)\n",
    "    #     loss_data = torch.mean((self.u_data - u_pred)**2.0) + torch.mean((self.v_data - v_pred)**2.0)\n",
    "    #     # compute the PBC loss\n",
    "    #     u_pbc1, v_pbc1 = self.NN_eval(self.t_pbc, self.x_pbc)\n",
    "    #     u_pbc2, v_pbc2 = self.NN_eval(self.t_pbc, self.x_pbc + self.period)\n",
    "    #     loss_pbc = torch.mean((u_pbc1 - u_pbc2)**2.0) + torch.mean((v_pbc1 - v_pbc2)**2.0)\n",
    "    #     # compute PDE loss\n",
    "    #     pde1_pred, pde2_pred = self.pde_eval(self.t_pde, self.x_pde)\n",
    "    #     loss_pde = torch.mean(pde1_pred ** 2) + torch.mean(pde2_pred ** 2)\n",
    "    #     # compute the total loss, it can be weighted\n",
    "    #     loss = loss_data + loss_pbc + loss_pde\n",
    "    #     # backward propagation\n",
    "    #     loss.backward()\n",
    "    #     # increase the iteration counter\n",
    "    #     self.iter += 1\n",
    "    #     # output\n",
    "    #     if (self.iter + 1) % 1000 == 0:\n",
    "    #         print(\n",
    "    #             'Iter %5d, Total: %10.4e, Data: %10.4e, PBC: %10.4e, PDE: %10.4e' \n",
    "    #             % (self.iter + 1, loss.item(), loss_data.item(), loss_pbc.item(), loss_pde.item())\n",
    "    #         )          \n",
    "    #     return loss\n",
    "    #\n",
    "    def train(self, nIter):\n",
    "        self.dnn.train()\n",
    "        for epoch in range(nIter):\n",
    "            # compute the data loss\n",
    "            u_pred, v_pred = self.NN_eval(self.t_data, self.x_data)\n",
    "            lws_IC = torch.pow(self.lws[:self.N_data, 0][:, None], 2.0)\n",
    "            loss_data = torch.mean(lws_IC * (self.u_data - u_pred)**2.0) + torch.mean(lws_IC * (self.v_data - v_pred)**2.0)\n",
    "            # compute the PBC loss\n",
    "            u_pbc1, v_pbc1 = self.NN_eval(self.t_pbc, self.x_pbc)\n",
    "            u_pbc2, v_pbc2 = self.NN_eval(self.t_pbc, self.x_pbc + self.period)\n",
    "            lws_pbc = torch.pow(self.lws[self.N_data : (self.N_data + self.N_pbc), 0][0, None], 2.0)\n",
    "            loss_pbc = torch.mean(lws_pbc * (u_pbc1 - u_pbc2)**2.0) + torch.mean(lws_pbc * (v_pbc1 - v_pbc2)**2.0)\n",
    "            # compute PDE loss\n",
    "            pde1_pred, pde2_pred = self.pde_eval(self.t_pde, self.x_pde)\n",
    "            lws_pde = torch.pow(self.lws[(self.N_data + self.N_pbc) : , 0][0, None], 2.0)\n",
    "            loss_pde = torch.mean(lws_pde * (pde1_pred ** 2)) + torch.mean(lws_pde * (pde2_pred ** 2))\n",
    "            # compute the total loss, it can be weighted\n",
    "            loss = loss_data + loss_pbc + loss_pde\n",
    "            # Backward and optimize\n",
    "            self.optimizer_Adam.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer_Adam.step()   \n",
    "            # update the loss weights\n",
    "            for lw in self.lws:\n",
    "                if lw.grad is not None:\n",
    "                    lw.grad.data.mul_(-1)\n",
    "            self.optimizer_lws.step()\n",
    "            # output the progress\n",
    "            if (epoch + 1) % 1000 == 0:\n",
    "                print(\n",
    "                    'Iter %5d, Total: %10.4e, Data: %10.4e, PBC: %10.4e, PDE: %10.4e' \n",
    "                    % (epoch + 1, loss.item(), loss_data.item(), loss_pbc.item(), loss_pde.item())\n",
    "                )\n",
    "            \n",
    "        \n",
    "    #        \n",
    "    def predict(self, X):\n",
    "        t = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device)\n",
    "        x = torch.tensor(X[:, 1:2], requires_grad=True).float().to(device)\n",
    "        self.dnn.eval()\n",
    "        u, v = self.NN_eval(t, x)\n",
    "        u = u.detach().cpu().numpy()\n",
    "        v = v.detach().cpu().numpy()\n",
    "        return u, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_IC = 512\n",
    "dt = 2\n",
    "N_PBC = int(thi/dt)\n",
    "# where we know the data\n",
    "# IC\n",
    "# IC: t = 0, x \\in [xlo, xhi], it can be on a gridd or random\n",
    "# make sure they are all column vectors, which is a 2-dim tensor\n",
    "# we will use chebyshev points\n",
    "cheb_pts = (xhi - xlo)/2.0 * np.cos((2.0 * np.arange(1, N_IC + 1) - 1.0)/N_IC * np.pi/2.0) + (xhi + xlo)/2.0\n",
    "xIC = np.expand_dims(cheb_pts, axis = 1)\n",
    "tIC = np.zeros_like(xIC)\n",
    "ptsIC = np.hstack((tIC, xIC))\n",
    "uIC = u0(xIC)\n",
    "vIC = v0(xIC)\n",
    "# PBC: x = xlo, t \\in [0, T], it can be on a grid or random\n",
    "tPBC = np.expand_dims(np.linspace(tlo, thi, N_PBC + 1)[1:], axis = 1)\n",
    "xPBC = xlo * np.ones_like(tPBC)\n",
    "ptsPBC = np.hstack((tPBC, xPBC))\n",
    "# assemble them (the data first)\n",
    "data_pts = ptsIC\n",
    "u_pts = uIC\n",
    "v_pts = vIC\n",
    "# not going to use random points, but use grided points\n",
    "# # collocation points for PDE loss\n",
    "# pts_PDE = lhs(2, N_PDE)\n",
    "# # transform it back to proper range\n",
    "# t_pde = tlo + (thi - tlo) * pts_PDE[:, 0:1]\n",
    "# x_pde = xlo + (xhi - xlo) * pts_PDE[:, 1:2]\n",
    "# pts_PDE = np.hstack((t_pde, x_pde))\n",
    "t = np.linspace(tlo, thi, N_PBC + 1)[1:] # not taking the initial time\n",
    "x = cheb_pts\n",
    "T, X = np.meshgrid(t, x)\n",
    "pts_PDE = np.hstack((T.flatten()[:, None], X.flatten()[:, None]))\n",
    "N_PDE = pts_PDE.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [2, 20, 20, 20, 20, 2]\n",
    "model = PhysicsInformedNN(data_pts, u_pts, v_pts, ptsPBC, period, pts_PDE, layers, epsilon1, epsilon2, b, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v6/m108yk7s0lxgfrf6_w_204dw0000gn/T/ipykernel_87703/4036735798.py:106: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  if lw.grad is not None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter  1000, Total: 8.9627e-06, Data: 6.1960e-06, PBC: 1.2601e-07, PDE: 2.6406e-06\n",
      "Iter  2000, Total: 2.1632e-04, Data: 7.2769e-05, PBC: 1.2234e-04, PDE: 2.1208e-05\n",
      "Iter  3000, Total: 3.5134e-05, Data: 2.1005e-05, PBC: 3.4842e-06, PDE: 1.0644e-05\n",
      "Iter  4000, Total: 3.2183e-04, Data: 1.2887e-04, PBC: 1.9285e-04, PDE: 1.0905e-07\n",
      "Iter  5000, Total: 2.6222e-04, Data: 1.1059e-04, PBC: 1.3972e-04, PDE: 1.1912e-05\n",
      "Iter  6000, Total: 2.5997e-04, Data: 1.5130e-04, PBC: 9.9342e-05, PDE: 9.3295e-06\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply PINN to the same grid as the quadrature solution for comparison\n",
    "t = np.linspace(tlo, thi, 101)\n",
    "x = np.linspace(xlo, xhi, 101)\n",
    "T, X = np.meshgrid(t, x)\n",
    "pts_flat = np.hstack((T.flatten()[:, None], X.flatten()[:, None]))\n",
    "u_pred, v_pred = model.predict(pts_flat)\n",
    "# #\n",
    "# Exact = u_quad.T\n",
    "# Exact_vec = Exact.flatten()[:, None]\n",
    "# error_u = np.linalg.norm(Exact_vec-u_pred,2)/np.linalg.norm(Exact_vec,2)\n",
    "# print('Error u: %e' % (error_u))                     \n",
    "u_pred = griddata(pts_flat, u_pred.flatten(), (T, X), method='cubic')\n",
    "v_pred = griddata(pts_flat, v_pred.flatten(), (T, X), method='cubic')\n",
    "# Error = np.abs(Exact - U_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The aesthetic setting has changed. \"\"\"\n",
    "\n",
    "####### Row 0: u(t,x) ##################    \n",
    "\n",
    "fig = plt.figure(figsize=(11, 5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "#\n",
    "ax.plot(ptsIC[:, 0], ptsIC[:, 1], \n",
    "    'kx', label = 'IC Data (%d points)' % (ptsIC.shape[0]), \n",
    "    markersize = 4,  # marker size doubled\n",
    "    clip_on = False,\n",
    "    alpha=1.0\n",
    ")\n",
    "#\n",
    "ax.plot(ptsPBC[:, 0], ptsPBC[:, 1], \n",
    "    'bo', label = 'LPBC Data (%d points)' % (ptsPBC.shape[0]), \n",
    "    markersize = 4,  # marker size doubled\n",
    "    clip_on = False,\n",
    "    alpha=1.0\n",
    ")\n",
    "#\n",
    "ax.plot(ptsPBC[:, 0], ptsPBC[:, 1] + period, \n",
    "    'bo', label = 'RPBC Data (%d points)' % (ptsPBC.shape[0]), \n",
    "    markersize = 4,  # marker size doubled\n",
    "    clip_on = False,\n",
    "    alpha=1.0\n",
    ")\n",
    "#\n",
    "ax.plot(pts_PDE[:, 0], pts_PDE[:, 1], \n",
    "    'rd', label = 'PDE Data (%d points)' % (pts_PDE.shape[0]), \n",
    "    markersize = 4,  # marker size doubled\n",
    "    clip_on = False,\n",
    "    alpha=1.0\n",
    ")\n",
    "#\n",
    "ax.set_xlabel('$t$', size=15)\n",
    "ax.set_ylabel('$x$', size=15)\n",
    "ax.legend(\n",
    "    loc='upper center', \n",
    "    bbox_to_anchor=(0.9, -0.05), \n",
    "    ncol=5, \n",
    "    frameon=False, \n",
    "    prop={'size': 15}\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_title('Points', fontsize = 15) # font size doubled\n",
    "ax.tick_params(labelsize=12)\n",
    "#\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Row 1: u(t,x) slices ################## \n",
    "\n",
    "\"\"\" The aesthetic setting has changed. \"\"\"\n",
    "\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "#\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "#ax.plot(x, Exact[:, 25], 'bo-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(x, u_pred[:, 0], 'rx--', linewidth = 2, label = 'u')\n",
    "ax.plot(x, v_pred[:, 0], 'bo--', linewidth = 2, label = 'v')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')    \n",
    "ax.set_title('$t = %.1f$' %(t[0]), fontsize = 15)\n",
    "#ax.axis('square')\n",
    "#ax.set_xlim([-1.1,1.1])\n",
    "#ax.set_ylim([-1.1,1.1])\n",
    "plt.locator_params(axis = 'y', nbins = 5)\n",
    "plt.locator_params(axis = 'x', nbins = 5)\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(15)\n",
    "\n",
    "ax = plt.subplot(1, 3, 2)\n",
    "#ax.plot(x,Exact[:,50], 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(x, u_pred[:, 50], 'rx--', linewidth = 2, label = 'u')\n",
    "ax.plot(x, v_pred[:, 50], 'bo--', linewidth = 2, label = 'v')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')\n",
    "#ax.axis('square')\n",
    "#ax.set_xlim([-1.1,1.1])\n",
    "#ax.set_ylim([-1.1,1.1])\n",
    "ax.set_title('$t = %.1f$' %(t[50]), fontsize = 15)\n",
    "ax.legend(\n",
    "    loc='upper center', \n",
    "    bbox_to_anchor=(0.5, -0.15), \n",
    "    ncol=5, \n",
    "    frameon=False, \n",
    "    prop={'size': 15}\n",
    ")\n",
    "plt.locator_params(axis = 'y', nbins = 5)\n",
    "plt.locator_params(axis = 'x', nbins = 5)\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(15)\n",
    "\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "#ax.plot(x,Exact[:,75], 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(x, u_pred[:, 100], 'rx--', linewidth = 2, label = 'u')\n",
    "ax.plot(x, v_pred[:, 100], 'bo--', linewidth = 2, label = 'v')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')\n",
    "#ax.axis('square')\n",
    "#ax.set_xlim([-1.1,1.1])\n",
    "#ax.set_ylim([-1.1,1.1])    \n",
    "ax.set_title('$t = %.1f$' %(t[100]), fontsize = 15)\n",
    "plt.locator_params(axis = 'y', nbins = 5)\n",
    "plt.locator_params(axis = 'x', nbins = 5)\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
